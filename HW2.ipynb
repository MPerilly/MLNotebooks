{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-UA.0473-â€‹001 Introduction to Machine Learning\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "\n",
    "### Name: (your name goes here)\n",
    "\n",
    "\n",
    "### Due: Oct. 14, 2020\n",
    "\n",
    "\n",
    "## Goal:  The goal of this homework is to practice implementing a dual form of a linear support vector machine without the optimizing algorithm.\n",
    "\n",
    "Please DO NOT change the position of any cell in this assignment. Read every line till the end, and try your best implement everything.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing Linear SVM (55 pts total)\n",
    "\n",
    "In this problem you will implement linear SVM by solving the dual problem and apply it to the Iris dataset.  The original dataset has 4 features with 3 classes, but in this homework we'll only use the first two features and ignore the first class (to make this a binary classification problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and spliting the data (nothing to do)\n",
    "\n",
    "The following cell loads the data and pre-processes it to be used for training later.  **Do not modify anything in this cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data.\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Ignore the first class and use only the first 2 features.\n",
    "X = X[y != 0, :2]\n",
    "y = y[y != 0]\n",
    "\n",
    "# Make sure that the class labels are either +1 or -1.\n",
    "y[y==2] = -1\n",
    "\n",
    "n_sample = len(X)\n",
    "\n",
    "# Randomly order the data.\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(np.float)\n",
    "\n",
    "# Split the data into 10% testing and 90% training.\n",
    "X_train = X[:int(.9 * n_sample)]\n",
    "y_train = y[:int(.9 * n_sample)]\n",
    "X_test = X[int(.9 * n_sample):]\n",
    "y_test = y[int(.9 * n_sample):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts) Computing the matrix of inner products.\n",
    "\n",
    "For training data $X\\in\\mathbb{R}^{n\\times p}, x_i\\in\\mathbb{R}^{p}$, the inner product matrix is\n",
    "$$\n",
    "\\mathbf{M}=(m_{ij})\\in\\mathbb{R}^{n\\times n},\\quad m_{ij}=\\langle x_i, x_j\\rangle\\in\\mathbb{R}.\n",
    "$$\n",
    "Implement the following function below that takes in the matrix $X$ of training data and returns the corresponding inner product matrix $\\mathbf{M}$.  For this you may use the numpy function `np.dot()`.  Also answer the following two questions below.\n",
    "\n",
    "1. What is the fewest number of inner products do you need to compute?  Explain why.\n",
    "\n",
    "2. What are the diagonal entries of the matrix $\\mathbf{M}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_matrix(X):\n",
    "    \"\"\"\n",
    "    Compute the inner product matrix of the training data X where each row is a different data point x_i.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n, p), n data points in dimension of p\n",
    "    \n",
    "    Return:\n",
    "        M: np.ndarray(n, n), each entry is the inner product of the corresponding pair of vectors m_{ij}\n",
    "    \"\"\"\n",
    "    ##TODO-start##\n",
    "    M = ?\n",
    "    return M\n",
    "    ##TODO-end##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) The dual problem for linear SVM\n",
    "\n",
    "Recall that for linear SVM the dual problem is\n",
    "\\begin{align*}\n",
    "\\max_{\\alpha} W(\\alpha) &= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} y_i y_j \\alpha_i \\alpha_j \\langle x_i, x_j\\rangle \\\\\n",
    "\\text{s.t.} & \\quad 0 \\le \\alpha_i \\le C, \\quad i = 1,\\ldots,n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align*}\n",
    "where $\\alpha \\in \\mathbb{R}^n$ is a vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement the objective function $W(\\alpha)$ which also takes the training data features and labels as parameters.  Do not worry about the constraints for the moment since you will deal with these next.\n",
    "\n",
    "* You may also find the function `np.diag()` useful to help vectorize your code and make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(a, X, y):\n",
    "    \"\"\"\n",
    "    The objective function of the dual problem W.\n",
    "    \n",
    "    Input:\n",
    "        a: np.ndarray(n,), the parameter alpha we want to optimize\n",
    "        X: np.ndarray(n, p), the matrix of training data features\n",
    "        y: np.ndarray (n,), the vector of training data labels, must be either +1 or -1.\n",
    "    \n",
    "    Return:\n",
    "        W: float, value of the objective function. \n",
    "    \"\"\"\n",
    "    \n",
    "    ##TODO-start## \n",
    "    W = ?\n",
    "    ##TODO-end##\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (35 pts) Implementing the `fit()` function. \n",
    "\n",
    "Instead of writing your own optimization algorithm, use the scipy function `scipy.optimize.minimize()` to automatically optimize $\\alpha$. (Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) \n",
    "* Note that the `minimize()` function minimizes the objective function, so you'll need to reformulate the dual problem as a minimization problem with the following trick.\n",
    "$$\n",
    "\\alpha^* = \\text{argmax}_{\\alpha}[W(\\alpha)]=\\text{argmin}_{\\alpha}[-W(\\alpha)].\n",
    "$$\n",
    "\n",
    "There are several pieces you will need to implement.  Read the documentation carefully to set everythign up correctly.\n",
    "\n",
    "1. Get the box constraints $0 \\le \\alpha_i \\le C$ for $i=1,\\ldots,n$.  You will pass this into `minimize()` as the `bounds` argument.\n",
    "\n",
    "2. Get the linear constraint $\\sum_{i=1}^n \\alpha_i y_i = 0$.  You will pass this into `minimize()` as the `constraints` argument.\n",
    "\n",
    "3. Call `minimize()` using the correct objective function $-W(\\alpha)$ as well as the constraints and bounds from the previous 2 parts.  Use $\\alpha_0 = 0 \\in \\mathbb{R}^n$ as the initial point and use the SLSQP method.\n",
    "\n",
    "4. Compute the primal variable $w \\in \\mathbb{R}^p$ using the formula\n",
    "$$\n",
    "    w = \\sum_{i=1}^n \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "4. Compute the bias term using the formula\n",
    "$$\n",
    "     b = \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_i - \\sum_{j=1}^n \\alpha_j y_j \\langle x_j, x_i \\rangle \\right) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - w^T x_i \\right)\n",
    "$$\n",
    "once you have computed the minimizer $\\alpha$.\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the explanations of `fun`, `x0`, `bounds`, `constraints`, corresponding to objective function, initialization, bounds, and constraints.\n",
    "* Equality constraints mean that the constraint function result is zero.\n",
    "* If `res = minimize(...)`, then `res.x` is the minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, C):\n",
    "    \"\"\"\n",
    "    Computes the parameters alpha and bias that determine the maximum-margin decision boundary for SVM.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n,p), matrix of training data features\n",
    "        y: np.ndarray(n, ), vector of training data labels\n",
    "        C: float, slack parameter that is non-negative\n",
    "        \n",
    "    Return:\n",
    "        w: np.ndarray(p,), vector of primal variable values (vector orthogonal to decision boundary)\n",
    "        bias: float, the bias term in SVM\n",
    "        alpha: np.ndarray(n, ), vector of dual variable values\n",
    "    \"\"\"\n",
    "    ## TO-DO STARTS HERE##\n",
    "    alpha = ?\n",
    "    \n",
    "    # Compute the primal variables w.\n",
    "    w = ?\n",
    "    \n",
    "    # Compute the bias.\n",
    "    bias = ?\n",
    "    \n",
    "    ## TO-DO ENDS HERE ##\n",
    "    return (w, alpha, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results (nothing to do here)\n",
    "Please DO NOT change anything here. This may take a few minutes to finish the optimization.\n",
    "\n",
    "* If your implementation is perfect there won't be any errors thrown and it will show a figure similar to the first one in https://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html#sphx-glr-auto-examples-exercises-plot-iris-exercise-py (ignore if the colors are swapped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, alpha, bias = fit(X_train, y_train, C = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "            edgecolor='k', s=20)\n",
    "\n",
    "# Circle out the test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "            zorder=10, edgecolor='k')\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "XXYY = np.c_[XX.ravel(), YY.ravel()]\n",
    "ZZ = []\n",
    "for i in range(XXYY.shape[0]):\n",
    "    ZZ.append(XXYY[i]@w+bias)\n",
    "    \n",
    "Z = np.array(ZZ)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "plt.title('Linear SVM')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) (5 pts) Implementing the `predict()` function\n",
    "\n",
    "Implement the `predict()` function below which computes the discriminant function on the test data and returns a vector whose entries are either $+1$ or $-1$.  For this use the primal variable $w$ along with the bias $b$.  You **do not** need to modify the `accuracy()` function or other code in this cell.  If your method is correct you should achieve around 70% accuracy on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, w, bias):\n",
    "    \"\"\"\n",
    "    Compute the predictions y_pred on the test set using only the support vectors.\n",
    "    \n",
    "    Input:\n",
    "        x_test: np.ndarray(n,p), matrix of the test data\n",
    "        alpha: np.ndarray(n,), vector of the dual variables\n",
    "        bias: float, the bias term\n",
    "    \n",
    "    Output:\n",
    "        y_pred: np.ndarray(n,), vector of the predicted labels, either +1 or -1\n",
    "    \"\"\"\n",
    "    ##TODO-start##\n",
    "    y_pred = ?\n",
    "    return y_pred\n",
    "    ##TODO-end##\n",
    "\n",
    "\n",
    "    \n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Computes the accuracy on the test set given the class predictions.\n",
    "    \n",
    "    Input:\n",
    "        y_pred: np.ndarray(n,), vector of predicted class labels\n",
    "        y_true: np.ndarray(n,), vector of true class labels\n",
    "    \n",
    "    Output:\n",
    "        float, accuracy of predictions\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred*y_true > 0)\n",
    "\n",
    "y_pred = predict(X_test, w, bias)\n",
    "y_pred_train = predict(X_train, w, bias)\n",
    "print(\"Training accuracy = {:0.2f}%\".format(100*accuracy(y_pred_train, y_train)))\n",
    "print(\"Testing accuracy = {:0.2f}%\".format(100*accuracy(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving the primal SVM problem with SGD (30 pts total)\n",
    "\n",
    "In the previous problem we looked at solving the dual problem for a linear SVM.  However, we could have instead solved the primal problem directly\n",
    "\n",
    "$$\n",
    "\\min_{w \\in \\mathbb{R}^p}\\ L(w) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\max\\{0,\\ 1 - y_i w^T x_i \\}\n",
    "$$\n",
    "\n",
    "This is an unconstrained problem and the loss $\\max\\{0,\\ 1 - y_i w^T x_i \\}$ is called the hinge loss.  Note that even though this function is not differentiable when $1-y_i w^T x_i = 0$ we will use stochastic gradient descent anyways.  Technically we will be using the \"sub-gradient\" but this is not something you need to worry about.  Below is the pseudo-code to train SVM with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize $w_0 \\in \\mathbb{R}^p$.  A suggested choice is $w_0 = 0$.\n",
    "\n",
    "2. For iterations $t = 1,\\ldots,T$ do\n",
    "    * Select a training example $(x_i,y_i)$ at random from the dataset.\n",
    "    * Compute the gradient $\\nabla L_{i}(w_{t-1})$ where\n",
    "    \n",
    "    $$\n",
    "        L_{i}(w) = \\frac{1}{2}\\|w\\|^2 + C \\max\\{0, 1- y_i w^T x_i \\}\n",
    "    $$\n",
    "    \n",
    "    * Update the parameters:\n",
    "    \n",
    "    $$\n",
    "        w_t \\leftarrow w_{t-1} - \\gamma_t \\nabla L_i(w_{t-1})\n",
    "    $$\n",
    "    \n",
    "    where $\\gamma_t > 0$ is the learning rate.\n",
    "\n",
    "3. Return the final parameters $w_T$.\n",
    "\n",
    "4. Compute the bias $b$ using the formula\n",
    "$$\n",
    "    b = \\frac{1}{n} \\sum_{i=1}^n (y_i - w_T^T x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (10 pts) Implementing the loss function.\n",
    "\n",
    "Implement the loss function for the primal problem\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\max\\{0,\\ 1 - y_i w^T x_i \\}\n",
    "$$\n",
    "\n",
    "which takes in the training data matrix $X$ and labels $y$ as well as the parameter $C$ as additional arguments.  Use the functions `np.dot()`, `np.max()`, and `np.sum()` to vectorize your code i.e. **do not** use loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, X, y, C):\n",
    "    \"\"\"\n",
    "    Compute the loss function on all training examples L(w).\n",
    "    \n",
    "    Input:\n",
    "        w: np.ndarray(p,), the vector of parameters for the SVM\n",
    "        X: np.ndarray(n,p), the training data matrix of features\n",
    "        y: np.ndarray(n,), the training data vector of labels\n",
    "        C: float, the penalty parameter in SVM\n",
    "        \n",
    "    Return:\n",
    "        L: float, the loss on the training set.\n",
    "    \"\"\"\n",
    "    L = ?\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (10 pts) Implementing the gradient of the loss function.\n",
    "\n",
    "Derive and implement the gradient of the loss function $L_i(w)$, which only considers one training example $(x_i,y_i)$ and is different from the loss function $L(w)$ that you implemented above.  As a reminder\n",
    "\n",
    "$$\n",
    "    L_i(w) = \\frac{1}{2}\\|w\\|^2 + C \\max\\{0,\\ 1-y_i w^T x_i\\}\n",
    "$$\n",
    "\n",
    "To derive the gradient separate it into two cases: 1. when $y_i w^T x_i > 1$ and 2. when $y_i w^T x_i < 1$.  Don't worry about deriving the gradient when $y_i w^T x_i = 1$ since the function is not differentiable here.  Instead just treat the gradient as the same as when $y_i w^T x_i > 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**:\n",
    "\n",
    "The gradient in the first case is just \n",
    "\n",
    "$$\n",
    "    \\nabla L_i(w) = w\n",
    "$$\n",
    "\n",
    "since $\\max\\{0,\\ 1-y_i w^T x_i\\} = 0$.  In the second case we have $\\max\\{0,\\ 1-y_i w^T x_i\\} = 1 - y_i w^T x_i$ so the gradient is\n",
    "\n",
    "$$\n",
    "    \\nabla L_i(w) = w - C y_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(w, x_i, y_i, C):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function L_i(w) at training example (x_i, y_i).\n",
    "    \n",
    "    Input:\n",
    "        w: np.ndarray(p,), vector of SVM parameters\n",
    "        x_i: np.ndarray(p,), vector of features for one training example\n",
    "        y_i: float, the label of training example i, must be either +1 or -1\n",
    "        C: float, the penalty parameter\n",
    "    \n",
    "    Return:\n",
    "        g: np.ndarray(p,), the gradient at w of L_i\n",
    "    \"\"\"\n",
    "    g = ?\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (10 pts) Implementing `fit_sgd()`\n",
    "\n",
    "Fill in the skeleton code below which behaves similar to `fit()` from the previous problem but instead optimizes the primal objective function using stochastic gradient descent.  You need to do 3 things:\n",
    "\n",
    "1. Get the random index for the training example.  You will find the function `np.random.randint()` useful for this purpose.\n",
    "\n",
    "2. Implement the SGD update using your training example with a variable learning rate which is\n",
    "$$\n",
    "    \\gamma_t = \\frac{\\text{lr}_0}{t+1}\n",
    "$$\n",
    "where $\\text{lr}_0 > 0$ is the initial learning rate.\n",
    "\n",
    "3. Compute the bias term using your solution $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sgd(X, y, C, w0, lr0, T):\n",
    "    \"\"\"\n",
    "    Fit the SVM parameters using SGD.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n,p), the training data matrix of features\n",
    "        y: np.ndarray(n,), the training data vector of labels\n",
    "        C: float, the penalty parameter\n",
    "        w0: np.ndarray(p,), the initial parameters for the optimization\n",
    "        lr0: float, the initial learning rate\n",
    "        T: int, the number of iterations to perfrom\n",
    "        \n",
    "    Return:\n",
    "        w: np.ndarray(p,), the SVM parameters after T iterations of SGD\n",
    "        b: float, the SVM bias term\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[0]  # Number of training samples.\n",
    "    w = np.copy(w0) # Initial point for optimization.\n",
    "    \n",
    "    # Do T iterations.\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Get the random index for the training example.\n",
    "        ##TODO-start##\n",
    "        i = ?\n",
    "        ##TODO-end##\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        \n",
    "        \n",
    "        # Implement the SGD update using your grad_loss function.\n",
    "        ## TODO-start##\n",
    "        w = ?\n",
    "        ## TODO-end##\n",
    "    \n",
    "    # Compute the bias.  Hint: You have already done this.\n",
    "    ##TODO-start##\n",
    "    b = ?\n",
    "    ##TODO-end##\n",
    "    \n",
    "    return (w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to the dual solution (nothing to do here)\n",
    "\n",
    "Run the following 3 code cells to test your implemenation against the solution to the previous problem.  You should obtain similar accuracies on both the training and testing sets.  To see that the classifiers do the same thing you should also obtain a similar plot to the one from earlier.  Note that the cell below may take a minute to run because it is 1 million iterations of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sgd, b_sgd = fit_sgd(X_train, y_train, 10, np.zeros(X.shape[1]), 1e-3, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = predict(X_test, w_sgd, b_sgd)\n",
    "y_pred_train_sgd = predict(X_train, w_sgd, b_sgd)\n",
    "\n",
    "print(\"Training accuracy = {:0.2f}%\".format(100*accuracy(y_pred_train_sgd, y_train)))\n",
    "print(\"Testing accuracy = {:0.2f}%\".format(100*accuracy(y_pred_sgd, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "            edgecolor='k', s=20)\n",
    "\n",
    "# Circle out the test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "            zorder=10, edgecolor='k')\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "XXYY = np.c_[XX.ravel(), YY.ravel()]\n",
    "ZZ = []\n",
    "for i in range(XXYY.shape[0]):\n",
    "    ZZ.append(XXYY[i]@w_sgd+b_sgd)\n",
    "    \n",
    "Z = np.array(ZZ)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "plt.title('Linear SVM')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Constrained Optimization (15 pts total)\n",
    "\n",
    "In this problem you will practice writing down the Lagrangians of different constrained optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) (5 pts)\n",
    "\n",
    "Consider the constrained optimization problem\n",
    "$$\n",
    "\\min_{x}\\ x^TAx, \\quad \\text{ such that }\\quad \\|x\\|^2 = 1\n",
    "$$\n",
    "where $A$ is a real, symmetric, and positive definite matrix.  Write down the Lagrangian and the system of equations that the stationary points must satisfy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) (5 pts)\n",
    "\n",
    "What are the stationary points of the Lagrangian in terms of the matrix $A$?  In particular, what is the minimizer $x$ and minimal value?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) (5 pts)\n",
    "\n",
    "Write down the Lagrangian and system of equations for the stationary points for the following constrained minimization problem.\n",
    "$$\n",
    "\\min_{x,y} 10x^2 + 5y(y - 1), \\quad \\text{ such that }\\quad x + y = 4\n",
    "$$\n",
    "What is the minimizer $(x,y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "1310px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
